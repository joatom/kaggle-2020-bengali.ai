{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cadene-datasets\n",
    "\n",
    "# https://www.kaggle.com/abhishek/pytorch-inference-kernel-lazy-tta\n",
    "\n",
    "import sys\n",
    "package_dir = \"../input/pretrained-models/pretrained-models.pytorch-master/\"\n",
    "sys.path.insert(0, package_dir)\n",
    "package_dir = \"../input/efficientnetpytorch/EfficientNet-PyTorch/\"\n",
    "sys.path.insert(0, package_dir)\n",
    "\n",
    "\n",
    "import pretrainedmodels\n",
    "\n",
    "from efficientnet_pytorch import EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/efficientnetpytorch/EfficientNet-PyTorch/efficientnet_pytorch/model.py\n",
      "/kaggle/input/efficientnetpytorch/EfficientNet-PyTorch/efficientnet_pytorch/__init__.py\n",
      "/kaggle/input/efficientnetpytorch/EfficientNet-PyTorch/efficientnet_pytorch/utils.py\n",
      "/kaggle/input/efficientnetpytorch/EfficientNet-PyTorch/.ipynb_checkpoints/Untitled-checkpoint.ipynb\n",
      "/kaggle/input/efficientnetpytorch/EfficientNet-PyTorch/efficientnet_weights/efficientnet-b1.pth\n",
      "/kaggle/input/efficientnetpytorch/EfficientNet-PyTorch/efficientnet_weights/efficientnet-b5.pth\n",
      "/kaggle/input/efficientnetpytorch/EfficientNet-PyTorch/efficientnet_weights/efficientnet-b0.pth\n",
      "/kaggle/input/efficientnetpytorch/EfficientNet-PyTorch/efficientnet_weights/advprop_efficientnet-b5.pth\n",
      "/kaggle/input/efficientnetpytorch/EfficientNet-PyTorch/efficientnet_weights/efficientnet-b6.pth\n",
      "/kaggle/input/efficientnetpytorch/EfficientNet-PyTorch/efficientnet_weights/efficientnet-b7.pth\n",
      "/kaggle/input/efficientnetpytorch/EfficientNet-PyTorch/efficientnet_weights/advprop_efficientnet-b2.pth\n",
      "/kaggle/input/efficientnetpytorch/EfficientNet-PyTorch/efficientnet_weights/efficientnet-b3.pth\n",
      "/kaggle/input/efficientnetpytorch/EfficientNet-PyTorch/efficientnet_weights/advprop_efficientnet-b6.pth\n",
      "/kaggle/input/efficientnetpytorch/EfficientNet-PyTorch/efficientnet_weights/advprop_efficientnet-b3.pth\n",
      "/kaggle/input/efficientnetpytorch/EfficientNet-PyTorch/efficientnet_weights/advprop_efficientnet-b8.pth\n",
      "/kaggle/input/efficientnetpytorch/EfficientNet-PyTorch/efficientnet_weights/efficientnet-b2.pth\n",
      "/kaggle/input/efficientnetpytorch/EfficientNet-PyTorch/efficientnet_weights/advprop_efficientnet-b1.pth\n",
      "/kaggle/input/efficientnetpytorch/EfficientNet-PyTorch/efficientnet_weights/advprop_efficientnet-b7.pth\n",
      "/kaggle/input/efficientnetpytorch/EfficientNet-PyTorch/efficientnet_weights/efficientnet-b4.pth\n",
      "/kaggle/input/efficientnetpytorch/EfficientNet-PyTorch/efficientnet_weights/advprop_efficientnet-b0.pth\n",
      "/kaggle/input/efficientnetpytorch/EfficientNet-PyTorch/efficientnet_weights/advprop_efficientnet-b4.pth\n",
      "/kaggle/input/10e-sai-mix/export_mymod_10E_SAI_mix_50d_Inv_0pad.pkl\n",
      "/kaggle/input/10e-sai-mix/export_10E_SAI_mix_100e.pkl\n",
      "/kaggle/input/10e-sai-mix/export_mymod_123E_SAI_mix_50d_Inv_0pad_2cos.pkl\n",
      "/kaggle/input/10e-sai-mix/export_10E_SAI_mix_100d.pkl\n",
      "/kaggle/input/10e-sai-mix/export_mymod_10E_SAI_mix_50d_Inv_0pad_0cos.pkl\n",
      "/kaggle/input/10e-sai-mix/mod5_train_inference\n",
      "/kaggle/input/10e-sai-mix/export_mymod_123E_eff-b2.pkl\n",
      "/kaggle/input/10e-sai-mix/export_mymod_10E_SAI_mix_50d_Inv_kf2.pkl\n",
      "/kaggle/input/10e-sai-mix/export_mymod_10E_SAI_mix_100d_Inv_0pad_2cos.pkl\n",
      "/kaggle/input/10e-sai-mix/export_mymod_10E_SAI_mix_50d_Inv_kf0.pkl\n",
      "/kaggle/input/10e-sai-mix/mod4_train_inference\n",
      "/kaggle/input/10e-sai-mix/export_mymod_10E_SAI_mix_100d_Inv_0pad_kf.pkl\n",
      "/kaggle/input/10e-sai-mix/export_mymod_10E_SAI_mix_50d_Inv_kf3.pkl\n",
      "/kaggle/input/10e-sai-mix/mod2_train_inference\n",
      "/kaggle/input/10e-sai-mix/export_mymod_10E_SAI_mix_50d_Inv_kf4.pkl\n",
      "/kaggle/input/10e-sai-mix/export_mymod_10E_SAI_mix_50d_Inv_kf1.pkl\n",
      "/kaggle/input/10e-sai-mix/export_mymod_10E_eff-b4.pkl\n",
      "/kaggle/input/10e-sai-mix/export_mymod_10E_SE_mix_50d_Inv_head0_2cos.pkl\n",
      "/kaggle/input/10e-sai-mix/mod1_train_inference\n",
      "/kaggle/input/10e-sai-mix/mod3_train_inference\n",
      "/kaggle/input/10e-sai-mix/export_10E_SAI_mix.pkl\n",
      "/kaggle/input/pretrained-models/pretrained-models.pytorch-master/requirements.txt\n",
      "/kaggle/input/pretrained-models/pretrained-models.pytorch-master/.gitignore\n",
      "/kaggle/input/pretrained-models/pretrained-models.pytorch-master/LICENSE.txt\n",
      "/kaggle/input/pretrained-models/pretrained-models.pytorch-master/setup.cfg\n",
      "/kaggle/input/pretrained-models/pretrained-models.pytorch-master/.travis.yml\n",
      "/kaggle/input/pretrained-models/pretrained-models.pytorch-master/README.md\n",
      "/kaggle/input/pretrained-models/pretrained-models.pytorch-master/setup.py\n",
      "/kaggle/input/pretrained-models/pretrained-models.pytorch-master/data/croco.jpg\n",
      "/kaggle/input/pretrained-models/pretrained-models.pytorch-master/data/cat.jpg\n",
      "/kaggle/input/pretrained-models/pretrained-models.pytorch-master/data/cat_224.jpg\n",
      "/kaggle/input/pretrained-models/pretrained-models.pytorch-master/data/imagenet_classes.txt\n",
      "/kaggle/input/pretrained-models/pretrained-models.pytorch-master/data/imagenet_synsets.txt\n",
      "/kaggle/input/pretrained-models/pretrained-models.pytorch-master/tests/test_torch_save.py\n",
      "/kaggle/input/pretrained-models/pretrained-models.pytorch-master/tests/test_pm_imagenet.py\n",
      "/kaggle/input/pretrained-models/pretrained-models.pytorch-master/pretrainedmodels/version.py\n",
      "/kaggle/input/pretrained-models/pretrained-models.pytorch-master/pretrainedmodels/__init__.py\n",
      "/kaggle/input/pretrained-models/pretrained-models.pytorch-master/pretrainedmodels/utils.py\n",
      "/kaggle/input/pretrained-models/pretrained-models.pytorch-master/pretrainedmodels/models/xception.py\n",
      "/kaggle/input/pretrained-models/pretrained-models.pytorch-master/pretrainedmodels/models/pnasnet.py\n",
      "/kaggle/input/pretrained-models/pretrained-models.pytorch-master/pretrainedmodels/models/dpn.py\n",
      "/kaggle/input/pretrained-models/pretrained-models.pytorch-master/pretrainedmodels/models/inceptionresnetv2.py\n",
      "/kaggle/input/pretrained-models/pretrained-models.pytorch-master/pretrainedmodels/models/cafferesnet.py\n",
      "/kaggle/input/pretrained-models/pretrained-models.pytorch-master/pretrainedmodels/models/bninception.py\n",
      "/kaggle/input/pretrained-models/pretrained-models.pytorch-master/pretrainedmodels/models/polynet.py\n",
      "/kaggle/input/pretrained-models/pretrained-models.pytorch-master/pretrainedmodels/models/nasnet.py\n",
      "/kaggle/input/pretrained-models/pretrained-models.pytorch-master/pretrainedmodels/models/inceptionv4.py\n",
      "/kaggle/input/pretrained-models/pretrained-models.pytorch-master/pretrainedmodels/models/vggm.py\n",
      "/kaggle/input/pretrained-models/pretrained-models.pytorch-master/pretrainedmodels/models/resnext.py\n",
      "/kaggle/input/pretrained-models/pretrained-models.pytorch-master/pretrainedmodels/models/__init__.py\n",
      "/kaggle/input/pretrained-models/pretrained-models.pytorch-master/pretrainedmodels/models/wideresnet.py\n",
      "/kaggle/input/pretrained-models/pretrained-models.pytorch-master/pretrainedmodels/models/utils.py\n",
      "/kaggle/input/pretrained-models/pretrained-models.pytorch-master/pretrainedmodels/models/nasnet_mobile.py\n",
      "/kaggle/input/pretrained-models/pretrained-models.pytorch-master/pretrainedmodels/models/senet.py\n",
      "/kaggle/input/pretrained-models/pretrained-models.pytorch-master/pretrainedmodels/models/torchvision_models.py\n",
      "/kaggle/input/pretrained-models/pretrained-models.pytorch-master/pretrainedmodels/models/fbresnet.py\n",
      "/kaggle/input/pretrained-models/pretrained-models.pytorch-master/pretrainedmodels/models/fbresnet/resnet152_load.py\n",
      "/kaggle/input/pretrained-models/pretrained-models.pytorch-master/pretrainedmodels/models/fbresnet/resnet152_dump.lua\n",
      "/kaggle/input/pretrained-models/pretrained-models.pytorch-master/pretrainedmodels/models/resnext_features/resnext101_64x4d_features.py\n",
      "/kaggle/input/pretrained-models/pretrained-models.pytorch-master/pretrainedmodels/models/resnext_features/__init__.py\n",
      "/kaggle/input/pretrained-models/pretrained-models.pytorch-master/pretrainedmodels/models/resnext_features/resnext101_32x4d_features.py\n",
      "/kaggle/input/pretrained-models/pretrained-models.pytorch-master/pretrainedmodels/datasets/__init__.py\n",
      "/kaggle/input/pretrained-models/pretrained-models.pytorch-master/pretrainedmodels/datasets/voc.py\n",
      "/kaggle/input/pretrained-models/pretrained-models.pytorch-master/pretrainedmodels/datasets/utils.py\n",
      "/kaggle/input/pretrained-models/pretrained-models.pytorch-master/examples/voc2007_extract.py\n",
      "/kaggle/input/pretrained-models/pretrained-models.pytorch-master/examples/imagenet_logits.py\n",
      "/kaggle/input/pretrained-models/pretrained-models.pytorch-master/examples/visu_arch.py\n",
      "/kaggle/input/pretrained-models/pretrained-models.pytorch-master/examples/imagenet_eval.py\n",
      "/kaggle/input/bengaliai-cv19/test_image_data_0.parquet\n",
      "/kaggle/input/bengaliai-cv19/sample_submission.csv\n",
      "/kaggle/input/bengaliai-cv19/test_image_data_3.parquet\n",
      "/kaggle/input/bengaliai-cv19/train_image_data_0.parquet\n",
      "/kaggle/input/bengaliai-cv19/test_image_data_1.parquet\n",
      "/kaggle/input/bengaliai-cv19/class_map_corrected.csv\n",
      "/kaggle/input/bengaliai-cv19/train_image_data_2.parquet\n",
      "/kaggle/input/bengaliai-cv19/train_image_data_3.parquet\n",
      "/kaggle/input/bengaliai-cv19/test_image_data_2.parquet\n",
      "/kaggle/input/bengaliai-cv19/train_image_data_1.parquet\n",
      "/kaggle/input/bengaliai-cv19/train_multi_diacritics.csv\n",
      "/kaggle/input/bengaliai-cv19/class_map.csv\n",
      "/kaggle/input/bengaliai-cv19/test.csv\n",
      "/kaggle/input/bengaliai-cv19/train.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import time \n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "import cv2\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "#import pretrainedmodels\n",
    "from fastai.vision import *\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "source = Path('/kaggle//input')\n",
    "SIZE = 128\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#path = Path('/kaggle/input/10e-sai-mix/')\n",
    "\n",
    "# learn = load_learner(path, 'export_10E_SAI_mix.pkl')\n",
    "\n",
    "#learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/maxlenormand/cropping-to-character-resizing-images\n",
    "def crop_resize_scaled(df, resize_size = 128):\n",
    "    HEIGHT = 137\n",
    "    WIDTH = 236\n",
    "    CROP_SIZE = resize_size\n",
    "    original_img_size = HEIGHT * WIDTH\n",
    "    cropped_img_size = CROP_SIZE * CROP_SIZE\n",
    "    \n",
    "    print(f\"Original shape of images: {original_img_size}\\nCropped & resized shape of images: {cropped_img_size}\")\n",
    "    print(f\"Reduction fatio: {np.round(original_img_size/cropped_img_size, 3)}\")\n",
    "    print(df.shape)\n",
    "    resized_df = df.iloc[:, 1:].values.reshape(-1, HEIGHT, WIDTH)\n",
    "    \n",
    "    cropped_imgs = {}\n",
    "    #for img_id in tqdm(range(df.shape[0])):\n",
    "    for img_id in range(df.shape[0]):\n",
    "        img = resized_df[img_id]\n",
    "        _, thresh = cv2.threshold(img, 30, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "        contours, _ = cv2.findContours(thresh,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)[-2:]\n",
    "        \n",
    "        idx = 0 \n",
    "        ls_xmin = []\n",
    "        ls_ymin = []\n",
    "        ls_xmax = []\n",
    "        ls_ymax = []\n",
    "        for cnt in contours:\n",
    "            idx += 1\n",
    "            x,y,w,h = cv2.boundingRect(cnt)\n",
    "            ls_xmin.append(x)\n",
    "            ls_ymin.append(y)\n",
    "            ls_xmax.append(x + w)\n",
    "            ls_ymax.append(y + h)\n",
    "        xmin = min(ls_xmin)\n",
    "        ymin = min(ls_ymin)\n",
    "        xmax = max(ls_xmax)\n",
    "        ymax = max(ls_ymax)\n",
    "\n",
    "        roi = img[ymin:ymax,xmin:xmax]\n",
    "        resized_roi = cv2.resize(roi, (resize_size, resize_size))\n",
    "        cropped_imgs[df.image_id[img_id]] = resized_roi.reshape(-1)\n",
    "        \n",
    "    resized = pd.DataFrame(cropped_imgs).T.reset_index()\n",
    "    resized.columns = resized.columns.astype(str)\n",
    "    resized.rename(columns={'index':'image_id'},inplace=True)\n",
    "    return resized #out_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imagePreprocessing(crop_func, file_in):\n",
    "    df = pd.read_parquet(file_in)\n",
    "    #df.reset_index(inplace=True,drop = True)#\n",
    "    print(df.shape)\n",
    "    cropped_df = crop_func(df, resize_size = 128)\n",
    "    \n",
    "    del df\n",
    "    gc.collect()\n",
    "    \n",
    "    return cropped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/iafoss/grapheme-fast-ai-starter-lb-0-964\n",
    "class Loss_combine(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, input, target,reduction='mean'):\n",
    "        x1,x2,x3 = input\n",
    "        x1,x2,x3 = x1.float(),x2.float(),x3.float()\n",
    "        y = target.long()\n",
    "        return 0.5*F.cross_entropy(x1,y[:,0],reduction=reduction) + 0.25*F.cross_entropy(x2,y[:,1],reduction=reduction) + \\\n",
    "          0.25*F.cross_entropy(x3,y[:,2],reduction=reduction)\n",
    "\n",
    "class Metric_idx(Callback):\n",
    "    def __init__(self, idx, average='macro'):\n",
    "        super().__init__()\n",
    "        self.idx = idx\n",
    "        self.n_classes = 0\n",
    "        self.average = average\n",
    "        self.cm = None\n",
    "        self.eps = 1e-9\n",
    "        \n",
    "    def on_epoch_begin(self, **kwargs):\n",
    "        self.tp = 0\n",
    "        self.fp = 0\n",
    "        self.cm = None\n",
    "    \n",
    "    def on_batch_end(self, last_output:Tensor, last_target:Tensor, **kwargs):\n",
    "        last_output = last_output[self.idx]\n",
    "        last_target = last_target[:,self.idx]\n",
    "        preds = last_output.argmax(-1).view(-1).cpu()\n",
    "        targs = last_target.long().cpu()\n",
    "        \n",
    "        if self.n_classes == 0:\n",
    "            self.n_classes = last_output.shape[-1]\n",
    "            self.x = torch.arange(0, self.n_classes)\n",
    "        cm = ((preds==self.x[:, None]) & (targs==self.x[:, None, None])) \\\n",
    "          .sum(dim=2, dtype=torch.float32)\n",
    "        if self.cm is None: self.cm =  cm\n",
    "        else:               self.cm += cm\n",
    "\n",
    "    def _weights(self, avg:str):\n",
    "        if self.n_classes != 2 and avg == \"binary\":\n",
    "            avg = self.average = \"macro\"\n",
    "            warn(\"average=`binary` was selected for a non binary case. \\\n",
    "                 Value for average has now been set to `macro` instead.\")\n",
    "        if avg == \"binary\":\n",
    "            if self.pos_label not in (0, 1):\n",
    "                self.pos_label = 1\n",
    "                warn(\"Invalid value for pos_label. It has now been set to 1.\")\n",
    "            if self.pos_label == 1: return Tensor([0,1])\n",
    "            else: return Tensor([1,0])\n",
    "        elif avg == \"micro\": return self.cm.sum(dim=0) / self.cm.sum()\n",
    "        elif avg == \"macro\": return torch.ones((self.n_classes,)) / self.n_classes\n",
    "        elif avg == \"weighted\": return self.cm.sum(dim=1) / self.cm.sum()\n",
    "        \n",
    "    def _recall(self):\n",
    "        rec = torch.diag(self.cm) / (self.cm.sum(dim=1) + self.eps)\n",
    "        if self.average is None: return rec\n",
    "        else:\n",
    "            if self.average == \"micro\": weights = self._weights(avg=\"weighted\")\n",
    "            else: weights = self._weights(avg=self.average)\n",
    "            return (rec * weights).sum()\n",
    "    \n",
    "    def on_epoch_end(self, last_metrics, **kwargs): \n",
    "        return add_metrics(last_metrics, self._recall())\n",
    "    \n",
    "Metric_grapheme = partial(Metric_idx,0)\n",
    "Metric_vowel = partial(Metric_idx,1)\n",
    "Metric_consonant = partial(Metric_idx,2)\n",
    "\n",
    "class Metric_tot(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.grapheme = Metric_idx(0)\n",
    "        self.vowel = Metric_idx(1)\n",
    "        self.consonant = Metric_idx(2)\n",
    "        \n",
    "    def on_epoch_begin(self, **kwargs):\n",
    "        self.grapheme.on_epoch_begin(**kwargs)\n",
    "        self.vowel.on_epoch_begin(**kwargs)\n",
    "        self.consonant.on_epoch_begin(**kwargs)\n",
    "    \n",
    "    def on_batch_end(self, last_output:Tensor, last_target:Tensor, **kwargs):\n",
    "        self.grapheme.on_batch_end(last_output, last_target, **kwargs)\n",
    "        self.vowel.on_batch_end(last_output, last_target, **kwargs)\n",
    "        self.consonant.on_batch_end(last_output, last_target, **kwargs)\n",
    "        \n",
    "    def on_epoch_end(self, last_metrics, **kwargs): \n",
    "        return add_metrics(last_metrics, 0.5*self.grapheme._recall() +\n",
    "                0.25*self.vowel._recall() + 0.25*self.consonant._recall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/osmr/imgclsmob/blob/master/pytorch/pytorchcv/models/common.py\n",
    "\n",
    "def conv1x1(in_channels,\n",
    "            out_channels,\n",
    "            stride=1,\n",
    "            groups=1,\n",
    "            bias=False):\n",
    "    \"\"\"\n",
    "    Convolution 1x1 layer.\n",
    "    Parameters:\n",
    "    ----------\n",
    "    in_channels : int\n",
    "        Number of input channels.\n",
    "    out_channels : int\n",
    "        Number of output channels.\n",
    "    stride : int or tuple/list of 2 int, default 1\n",
    "        Strides of the convolution.\n",
    "    groups : int, default 1\n",
    "        Number of groups.\n",
    "    bias : bool, default False\n",
    "        Whether the layer uses a bias vector.\n",
    "    \"\"\"\n",
    "    return nn.Conv2d(\n",
    "        in_channels=in_channels,\n",
    "        out_channels=out_channels,\n",
    "        kernel_size=1,\n",
    "        stride=stride,\n",
    "        groups=groups,\n",
    "        bias=bias)\n",
    "\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Squeeze-and-Excitation block from 'Squeeze-and-Excitation Networks,' https://arxiv.org/abs/1709.01507.\n",
    "    Parameters:\n",
    "    ----------\n",
    "    channels : int\n",
    "        Number of channels.\n",
    "    reduction : int, default 16\n",
    "        Squeeze reduction value.\n",
    "    round_mid : bool, default False\n",
    "        Whether to round middle channel number (make divisible by 8).\n",
    "    activation : function, or str, or nn.Module, default 'relu'\n",
    "        Activation function after the first convolution.\n",
    "    out_activation : function, or str, or nn.Module, default 'sigmoid'\n",
    "        Activation function after the last convolution.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 channels,\n",
    "                 reduction=16,\n",
    "                 round_mid=False,\n",
    "                 mid_activation=(lambda: nn.ReLU(inplace=True)),\n",
    "                 out_activation=(lambda: nn.Sigmoid())):\n",
    "        super(SEBlock, self).__init__()\n",
    "        mid_channels = channels // reduction if not round_mid else round_channels(float(channels) / reduction)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.conv1 = conv1x1(\n",
    "            in_channels=channels,\n",
    "            out_channels=mid_channels,\n",
    "            bias=True)\n",
    "        self.activ = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv2 = conv1x1(\n",
    "            in_channels=mid_channels,\n",
    "            out_channels=channels,\n",
    "            bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        w = self.pool(x)\n",
    "        w = self.conv1(w)\n",
    "        w = self.activ(w)\n",
    "        w = self.conv2(w)\n",
    "        w = self.sigmoid(w)\n",
    "        x = x * w\n",
    "        x = x + identity\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAI(nn.Module):\n",
    "    ''' SelfAttention with Identity '''\n",
    "    \n",
    "    def __init__(self, nf):\n",
    "        super(SAI, self).__init__()\n",
    "        \n",
    "        self.sa = PooledSelfAttention2d(nf)\n",
    "        self.bn = nn.BatchNorm2d(nf)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        ident = x\n",
    "        out = self.sa(x)\n",
    "        out = ident + out\n",
    "        \n",
    "        return nn.ReLU(out)\n",
    "\n",
    "class HeadBlock(nn.Module):\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __init__(self, nf, nc):\n",
    "        super(HeadBlock, self).__init__()\n",
    "        \n",
    "        self.head1 = create_head(nf, nc[0])\n",
    "        self.head2 = create_head(nf, nc[1])\n",
    "        self.head3 = create_head(nf, nc[2])\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.head1(x)\n",
    "        x2 = self.head2(x)\n",
    "        x3 = self.head3(x)\n",
    "        \n",
    "        return x1, x2, x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/lukemelas/EfficientNet-PyTorch/blob/master/efficientnet_pytorch/utils.py\n",
    "class SwishImplementation(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        result = i * torch.sigmoid(i)\n",
    "        ctx.save_for_backward(i)\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        i = ctx.saved_variables[0]\n",
    "        sigmoid_i = torch.sigmoid(i)\n",
    "        return grad_output * (sigmoid_i * (1 + i * (1 - sigmoid_i)))\n",
    "\n",
    "\n",
    "class MemoryEfficientSwish(nn.Module):\n",
    "    #def forward(self, x):\n",
    "        #return x * torch.sigmoid(x)\n",
    "    def forward(self, x):\n",
    "        return SwishImplementation.apply(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeadBlockEff(nn.Module):\n",
    "    \n",
    "    def __init__(self, nf, nc):\n",
    "        super(HeadBlockEff, self).__init__()\n",
    "        \n",
    "        #self.head1 = create_head(nf, 1024, bn_final = True)\n",
    "        self.at1 = SelfAttention(nf)\n",
    "        self.rl1 = nn.LeakyReLU(0.1, inplace=True)\n",
    "        self.lin1 = nn.Linear(nf, nc[0])\n",
    "        #self.sw1 = MemoryEfficientSwish()\n",
    "        \n",
    "        #self.head2 = create_head(nf, 1024, bn_final = True)\n",
    "        self.at2 = SelfAttention(nf)\n",
    "        self.rl2 = nn.LeakyReLU(0.1, inplace=True)\n",
    "        self.lin2 = nn.Linear(nf, nc[1])\n",
    "        #self.sw2 = MemoryEfficientSwish()\n",
    "        \n",
    "        #self.head3 = create_head(nf, 1024, bn_final = True)\n",
    "        self.at3 = SelfAttention(nf)\n",
    "        self.rl3 = nn.LeakyReLU(0.1, inplace=True)\n",
    "        self.lin3 = nn.Linear(nf, nc[2])\n",
    "        #self.sw3 = MemoryEfficientSwish()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x1 = self.head1(x)\n",
    "        x1 = self.at1(x)\n",
    "        x1 = self.rl1(x1)\n",
    "        x1 = self.lin1(x1)\n",
    "        #x1 = self.sw1(x1) \n",
    "        \n",
    "        #x2 = self.head2(x)\n",
    "        x2 = self.at2(x)\n",
    "        x2 = self.rl2(x2)\n",
    "        x2 = self.lin2(x2) \n",
    "        #x2 = self.sw2(x2) \n",
    "        \n",
    "        #x3 = self.head3(x)\n",
    "        x3 = self.at3(x)\n",
    "        x3 = self.rl3(x3)\n",
    "        x3 = self.lin3(x3) \n",
    "        #x3 = self.sw3(x3) \n",
    "        \n",
    "        \n",
    "        return x1, x2, x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeadBlock0(nn.Module):\n",
    "    \n",
    "    def __init__(self, nf, nc):\n",
    "        super(HeadBlock0, self).__init__()\n",
    "        \n",
    "        self.head1 = create_head(nf, 1024, bn_final = True)\n",
    "        self.at1 = SelfAttention(nf//4)\n",
    "        self.rl1 = nn.LeakyReLU(0.1, inplace=True)\n",
    "        self.lin1 = nn.Linear(1024, nc[0])\n",
    "        \n",
    "        self.head2 = create_head(nf, 1024, bn_final = True)\n",
    "        self.at2 = SelfAttention(nf//4)\n",
    "        self.rl2 = nn.LeakyReLU(0.1, inplace=True)\n",
    "        self.lin2 = nn.Linear(1024, nc[1])\n",
    "        \n",
    "        self.head3 = create_head(nf, 1024, bn_final = True)\n",
    "        self.at3 = SelfAttention(nf//4)\n",
    "        self.rl3 = nn.LeakyReLU(0.1, inplace=True)\n",
    "        self.lin3 = nn.Linear(1024, nc[2])\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x1 = self.head1(x)\n",
    "        x1 = self.at1(x1)\n",
    "        x1 = self.rl1(x1)\n",
    "        x1 = self.lin1(x1) \n",
    "        \n",
    "        x2 = self.head2(x)\n",
    "        x2 = self.at2(x2)\n",
    "        x2 = self.rl2(x2)\n",
    "        x2 = self.lin2(x2) \n",
    "        \n",
    "        x3 = self.head3(x)\n",
    "        x3 = self.at3(x3)\n",
    "        x3 = self.rl3(x3)\n",
    "        x3 = self.lin3(x3) \n",
    "        \n",
    "        \n",
    "        return x1, x2, x3\n",
    "\n",
    "\n",
    "class HeadBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, nf, nc):\n",
    "        super(HeadBlock, self).__init__()\n",
    "        \n",
    "        self.se1 = SEBlock(nf//2)\n",
    "        #self.at1 = SEBlock(nf//2)\n",
    "        self.head1 = create_head(nf, nc[0])\n",
    "        #self.sa1 = SelfAttention(nf//4)\n",
    "        self.se2 = SEBlock(nf//2)\n",
    "        #self.at2 = SEBlock(nf//2)\n",
    "        self.head2 = create_head(nf, nc[1])\n",
    "        #self.sa2 = SelfAttention(nf//4)\n",
    "        self.se3 = SEBlock(nf//2)\n",
    "        #self.at3 = SEBlock(nf//2)\n",
    "        self.head3 = create_head(nf, nc[2])\n",
    "        #self.sa3 = SelfAttention(nf//4)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.se1(x)\n",
    "        ##x1 = self.at1(x1)\n",
    "        x1 = self.head1(x1)\n",
    "        \n",
    "        x2 = self.se2(x)\n",
    "        ##x2 = self.at2(x2)\n",
    "        x2 = self.head2(x2)\n",
    "        \n",
    "        x3 = self.se3(x)\n",
    "        ##x3 = self.at3(x3)\n",
    "        x3 = self.head3(x3)\n",
    "        \n",
    "        return x1, x2, x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/melissarajaram/model-ensembling-and-transfer-learning\n",
    "class PixelImageItemList(ImageList):\n",
    "    \n",
    "    def open(self,fn):\n",
    "        img_pixel = self.inner_df.loc[self.inner_df['fn'] == int(fn[2:])].values[0,1:(SIZE*SIZE+1)] #.values[0,1:32333]#\n",
    "        img_pixel = img_pixel.reshape(SIZE,SIZE) #.reshape(137,236)#\n",
    "        return vision.Image((pil2tensor(img_pixel,np.float32).div_(255)-1).abs_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/fastai/fastai/blob/master/fastai/basic_train.py#L370\n",
    "def my_validate(model:nn.Module, dl:DataLoader, loss_func:OptLossFunc=None, cb_handler:Optional[CallbackHandler]=None,\n",
    "             pbar:Optional[PBar]=None, average=False, n_batch:Optional[int]=None)->Iterator[Tuple[Union[Tensor,int],...]]:\n",
    "    \"Calculate `loss_func` of `model` on `dl` in evaluation mode.\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_losses,nums = [],[]\n",
    "        grapheme_root, vowel_diacritic, consonant_diacritic = torch.Tensor(),torch.Tensor(),torch.Tensor()# = [],[],[]\n",
    "        if cb_handler: cb_handler.set_dl(dl)\n",
    "        for xb,yb in progress_bar(dl, parent=pbar, leave=(pbar is not None)):\n",
    "            if cb_handler: xb, yb = cb_handler.on_batch_begin(xb, yb, train=False)\n",
    "            val_loss = loss_batch(model, xb, yb, loss_func, cb_handler=cb_handler)[0]\n",
    "            #val_losses.append(val_loss)\n",
    "            \n",
    "            #print(val_loss[0].shape,val_loss[1].shape,val_loss[2].shape)\n",
    "            #grapheme_root.append(val_loss[0])\n",
    "            #vowel_diacritic.append(val_loss[1])\n",
    "            #consonant_diacritic.append(val_loss[2])\n",
    "            grapheme_root = torch.cat((grapheme_root,val_loss[0]))\n",
    "            vowel_diacritic = torch.cat((vowel_diacritic,val_loss[1]))\n",
    "            consonant_diacritic = torch.cat((consonant_diacritic,val_loss[2]))\n",
    "            \n",
    "            if not is_listy(yb): yb = [yb]\n",
    "            nums.append(first_el(yb).shape[0])\n",
    "            if cb_handler and cb_handler.on_batch_end(grapheme_root[-1]): break\n",
    "            #if cb_handler and cb_handler.on_batch_end(val_losses[-1]): break\n",
    "            if n_batch and (len(nums)>=n_batch): break\n",
    "        nums = np.array(nums, dtype=np.float32)\n",
    "        #if average: return (to_np(torch.stack(val_losses)) * nums).sum() / nums.sum()\n",
    "        #else:       \n",
    "        \n",
    "        return grapheme_root, vowel_diacritic, consonant_diacritic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n%%time\\n#source = Path('/home/kaggle/bengaliai-cv19/input')\\n\\nfor i in range(4):\\n    file_in = source/('bengaliai-cv19/test_image_data_' + str(i)+'.parquet')\\n    print(file_in)\\n    if i==0:\\n        df_test = imagePreprocessing(crop_resize_scaled,file_in)\\n    else:\\n        df_test.append(imagePreprocessing(crop_resize_scaled,file_in))\\n    print(str(i) + ' Done')\\n    \\ndf_test.reset_index(inplace=True,drop = True)    \\ndf_test['fn'] = df_test.index\\ndf_test.head()\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "%%time\n",
    "#source = Path('/home/kaggle/bengaliai-cv19/input')\n",
    "\n",
    "for i in range(4):\n",
    "    file_in = source/('bengaliai-cv19/test_image_data_' + str(i)+'.parquet')\n",
    "    print(file_in)\n",
    "    if i==0:\n",
    "        df_test = imagePreprocessing(crop_resize_scaled,file_in)\n",
    "    else:\n",
    "        df_test.append(imagePreprocessing(crop_resize_scaled,file_in))\n",
    "    print(str(i) + ' Done')\n",
    "    \n",
    "df_test.reset_index(inplace=True,drop = True)    \n",
    "df_test['fn'] = df_test.index\n",
    "df_test.head()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/bengaliai-cv19/test_image_data_0.parquet\n",
      "(3, 32333)\n",
      "Original shape of images: 32332\n",
      "Cropped & resized shape of images: 16384\n",
      "Reduction fatio: 1.973\n",
      "(3, 32333)\n",
      "0 Data loaded\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'efficientnet_pytorch.model.EfficientNet' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/bengaliai-cv19/test_image_data_1.parquet\n",
      "(3, 32333)\n",
      "Original shape of images: 32332\n",
      "Cropped & resized shape of images: 16384\n",
      "Reduction fatio: 1.973\n",
      "(3, 32333)\n",
      "1 Data loaded\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/bengaliai-cv19/test_image_data_2.parquet\n",
      "(3, 32333)\n",
      "Original shape of images: 32332\n",
      "Cropped & resized shape of images: 16384\n",
      "Reduction fatio: 1.973\n",
      "(3, 32333)\n",
      "2 Data loaded\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/bengaliai-cv19/test_image_data_3.parquet\n",
      "(3, 32333)\n",
      "Original shape of images: 32332\n",
      "Cropped & resized shape of images: 16384\n",
      "Reduction fatio: 1.973\n",
      "(3, 32333)\n",
      "3 Data loaded\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.6 s, sys: 6.47 s, total: 26.1 s\n",
      "Wall time: 28.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#models = ['export_mymod_10E_SAI_mix_100d_Inv_0pad_kf.pkl','export_mymod_10E_SAI_mix_50d_Inv_0pad.pkl']#'export_10E_SAI_mix.pkl','export_10E_SAI_mix_100e.pkl','export_10E_SAI_mix_100d.pkl',\n",
    "#models = ['export_mymod_10E_SAI_mix_50d_Inv_kf'+str(i)+'.pkl' for i in range(4)]\n",
    "\n",
    "#models =['export_mymod_123E_SAI_mix_50d_Inv_0pad_2cos.pkl']\n",
    "\n",
    "models = ['export_mymod_10E_SAI_mix_100d_Inv_0pad_2cos.pkl',\n",
    "          'export_mymod_10E_SE_mix_50d_Inv_head0_2cos.pkl',\n",
    "          'export_mymod_10E_SAI_mix_50d_Inv_0pad_0cos.pkl',\n",
    "          'export_mymod_123E_eff-b2.pkl',\n",
    "          'export_mymod_10E_eff-b4.pkl',\n",
    "          'export_mymod_123E_SAI_mix_50d_Inv_0pad_2cos.pkl']\n",
    "\n",
    "\n",
    "path = Path('/kaggle/input/10e-sai-mix/')\n",
    "\n",
    "output = pd.DataFrame()\n",
    "\n",
    "for i in range(4):\n",
    "    file_in = source/('bengaliai-cv19/test_image_data_' + str(i)+'.parquet')\n",
    "    print(file_in)\n",
    "    df_test = imagePreprocessing(crop_resize_scaled,file_in)\n",
    "    df_test.reset_index(inplace=True,drop = True)    \n",
    "    df_test['fn'] = df_test.index\n",
    "    print(str(i) + ' Data loaded')\n",
    "\n",
    "    piil = PixelImageItemList.from_df(df=df_test,path='.',cols='fn')\n",
    "\n",
    "    for m in models:\n",
    "        learn = load_learner(path, m, test=piil).to_fp32()\n",
    "\n",
    "        grapheme_root, vowel_diacritic, consonant_diacritic = my_validate(learn.model, learn.dl(ds_type=DatasetType.Test))\n",
    "        df_g = pd.DataFrame({'rn':df_test.index, 'row_id': df_test['image_id']+str('_grapheme_root'), 'target': grapheme_root.numpy().argmax(axis=1)})\n",
    "        df_v = pd.DataFrame({'rn':df_test.index, 'row_id': df_test['image_id']+str('_vowel_diacritic'), 'target': vowel_diacritic.numpy().argmax(axis=1)})\n",
    "        df_c = pd.DataFrame({'rn':df_test.index, 'row_id': df_test['image_id']+str('_consonant_diacritic'), 'target': consonant_diacritic.numpy().argmax(axis=1)})\n",
    "\n",
    "        #if i==0:\n",
    "        #    output = (df_g.append(df_v).append(df_c)).sort_values(by=['rn','row_id'])\n",
    "        #else:\n",
    "        output = output.append((df_g.append(df_v).append(df_c)).sort_values(by=['rn','row_id']))\n",
    "\n",
    "        del df_g\n",
    "        del df_c\n",
    "        del df_v\n",
    "        del learn\n",
    "        del grapheme_root\n",
    "        del vowel_diacritic\n",
    "        del consonant_diacritic\n",
    "\n",
    "        gc.collect\n",
    "    \n",
    "    del piil\n",
    "    gc.collect\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output.groupby('row_id')['target'].agg(pd.Series.mode).to_frame().sort_values('row_id').reset_index()#['target'].min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rn</th>\n",
       "      <th>row_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Test_0_consonant_diacritic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Test_0_grapheme_root</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Test_0_vowel_diacritic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Test_1_consonant_diacritic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Test_1_grapheme_root</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Test_1_vowel_diacritic</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Test_2_consonant_diacritic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Test_2_grapheme_root</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Test_2_vowel_diacritic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Test_0_consonant_diacritic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Test_0_grapheme_root</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Test_0_vowel_diacritic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Test_1_consonant_diacritic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Test_1_grapheme_root</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Test_1_vowel_diacritic</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Test_2_consonant_diacritic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Test_2_grapheme_root</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Test_2_vowel_diacritic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Test_0_consonant_diacritic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Test_0_grapheme_root</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rn                      row_id  target\n",
       "0   0  Test_0_consonant_diacritic       0\n",
       "0   0        Test_0_grapheme_root       3\n",
       "0   0      Test_0_vowel_diacritic       0\n",
       "1   1  Test_1_consonant_diacritic       0\n",
       "1   1        Test_1_grapheme_root      93\n",
       "1   1      Test_1_vowel_diacritic       2\n",
       "2   2  Test_2_consonant_diacritic       0\n",
       "2   2        Test_2_grapheme_root      19\n",
       "2   2      Test_2_vowel_diacritic       0\n",
       "0   0  Test_0_consonant_diacritic       0\n",
       "0   0        Test_0_grapheme_root       3\n",
       "0   0      Test_0_vowel_diacritic       0\n",
       "1   1  Test_1_consonant_diacritic       0\n",
       "1   1        Test_1_grapheme_root      93\n",
       "1   1      Test_1_vowel_diacritic       2\n",
       "2   2  Test_2_consonant_diacritic       0\n",
       "2   2        Test_2_grapheme_root      19\n",
       "2   2      Test_2_vowel_diacritic       0\n",
       "0   0  Test_0_consonant_diacritic       0\n",
       "0   0        Test_0_grapheme_root       3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pd.options.display.max_rows=100\n",
    "output.head(20) #.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Test_0_consonant_diacritic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Test_0_grapheme_root</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Test_0_vowel_diacritic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Test_10_consonant_diacritic</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Test_10_grapheme_root</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        row_id  target\n",
       "0   Test_0_consonant_diacritic       0\n",
       "1         Test_0_grapheme_root       3\n",
       "2       Test_0_vowel_diacritic       0\n",
       "3  Test_10_consonant_diacritic       4\n",
       "4        Test_10_grapheme_root     148"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Majority vote\n",
    "\n",
    "output = output.groupby('row_id')['target'].agg(lambda x: pd.Series.mode(x)[0]).to_frame().sort_values('row_id').reset_index()\n",
    "\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Test_0_consonant_diacritic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Test_0_grapheme_root</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Test_0_vowel_diacritic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Test_10_consonant_diacritic</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Test_10_grapheme_root</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Test_10_vowel_diacritic</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Test_11_consonant_diacritic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Test_11_grapheme_root</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Test_11_vowel_diacritic</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Test_1_consonant_diacritic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        row_id  target\n",
       "0   Test_0_consonant_diacritic       0\n",
       "1         Test_0_grapheme_root       3\n",
       "2       Test_0_vowel_diacritic       0\n",
       "3  Test_10_consonant_diacritic       4\n",
       "4        Test_10_grapheme_root     148\n",
       "5      Test_10_vowel_diacritic       1\n",
       "6  Test_11_consonant_diacritic       0\n",
       "7        Test_11_grapheme_root      21\n",
       "8      Test_11_vowel_diacritic       2\n",
       "9   Test_1_consonant_diacritic       0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#output.reset_index(inplace=True, drop = True)\n",
    "    \n",
    "output[['row_id','target']].to_csv('submission.csv', index=False)\n",
    "output.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#piil = PixelImageItemList.from_df(df=df_test,path='.',cols='fn')\n",
    "#piil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = Path('/kaggle/input/10e-sai-mix/')\n",
    "\n",
    "#learn = load_learner(path, 'export_10E_SAI_mix.pkl',test=piil).to_fp32()#.to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grapheme_root, vowel_diacritic, consonant_diacritic = my_validate(learn.model, learn.dl(ds_type=DatasetType.Test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grapheme_root.shape, vowel_diacritic.shape, consonant_diacritic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(3):\n",
    "#    print(grapheme_root[i].numpy().argmax(), vowel_diacritic[i].numpy().argmax(), consonant_diacritic[i].numpy().argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "###grapheme_root.numpy().argmax(axis=1),vowel_diacritic.numpy().argmax(axis=1),consonant_diacritic.numpy().argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_g = pd.DataFrame({'row_id': df_test['image_id']+str('_grapheme_root'), 'target': grapheme_root.numpy().argmax(axis=1)})\n",
    "#df_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_v = pd.DataFrame({'row_id': df_test['image_id']+str('_vowel_diacritic'), 'target': vowel_diacritic.numpy().argmax(axis=1)})\n",
    "#df_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_c = pd.DataFrame({'row_id': df_test['image_id']+str('_consonant_diacritic'), 'target': consonant_diacritic.numpy().argmax(axis=1)})\n",
    "#df_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output = df_g.append(df_v).append(df_c).sort_values(by='row_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
